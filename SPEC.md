# Hedonic Compass â€” Technical Specification

> A personal experience tracking system that learns what makes you thrive.

**Version:** 1.0 | **Date:** February 2026  
**Stack:** TypeScript Â· Telegram Bot Â· Supabase Â· Claude API Â· Railway

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [System Architecture](#2-system-architecture)
3. [Database Schema](#3-database-schema)
4. [Project Structure](#4-project-structure)
5. [Claude Extraction Prompt](#5-claude-extraction-prompt)
6. [Weekly Digest](#6-weekly-digest)
7. [Environment Variables](#7-environment-variables)
8. [Deployment: Railway](#8-deployment-railway)
9. [Recommended Build Phases](#9-recommended-build-phases)
10. [Open Questions & Future Decisions](#10-open-questions--future-decisions)

---

## 1. Project Overview

Hedonic Compass is a personal experience tracking system that helps you learn what genuinely makes you thrive â€” and what drains you. Over time, it builds a structured picture of your hedonic profile and delivers weekly, actionable insights so you can deliberately design a life with more of what you love.

### 1.1 Core Philosophy

Most journaling tools optimize for capture. Hedonic Compass optimizes for **learning**. The goal is not to record your life â€” it's to understand it. Every entry is raw data that feeds a growing model of your personal flourishing.

> **Vision:** You speak a few sentences into your phone. The system silently extracts structure, stores it, and every Sunday delivers a digest that tells you exactly what to do more of â€” and less of â€” in the coming week.

### 1.2 Key Design Principles

- **Zero friction** â€” logging must feel as natural as texting a friend
- **Invisible structure** â€” the system extracts categories, intensity, and sentiment automatically; the user never fills out a form
- **Actionable output** â€” insights drive behavior change, not just reflection
- **Private by default** â€” all data belongs to the user, stored in their own Supabase instance

---

## 2. System Architecture

### 2.1 High-Level Overview

```
User speaks â†’ Wispr Flow transcribes â†’ Telegram message
    â†’ Grammy bot receives text
    â†’ Claude API extracts structure (JSON)
    â†’ Supabase stores raw + structured entry
    â†’ Bot sends "âœ“ Logged"

Every Sunday 7pm:
    â†’ Cron fires â†’ Query Supabase (last 7 days)
    â†’ Claude synthesizes digest
    â†’ Send via Telegram + Email
```

### 2.2 Technology Stack

| Layer | Technology |
|---|---|
| Bot Framework | [Grammy](https://grammy.dev/) â€” modern Telegram bot framework for Node.js |
| Runtime | Node.js 20 LTS + TypeScript |
| AI / Extraction | Anthropic Claude API (`claude-haiku-4-5` for extraction, `claude-opus-4-5` for digest) |
| Database | Supabase (PostgreSQL) |
| Hosting | Railway (auto-deploy from GitHub) |
| Scheduler | `node-cron` within the Railway service |
| Email | [Resend](https://resend.com) â€” transactional email API, generous free tier |

### 2.3 Data Flow: Logging an Entry

Every time you send a message to the bot:

1. User sends text via Telegram (typed or via Wispr Flow voice-to-text)
2. Grammy bot receives message, extracts `text` + `telegram_user_id` + `timestamp`
3. Bot calls Claude API with extraction prompt + raw text
4. Claude returns structured JSON: `sentiment`, `intensity`, `category`, `keywords`, etc.
5. Bot stores both `raw_text` AND extracted fields in Supabase
6. Bot sends a brief acknowledgment back to user (e.g. `âœ“ Logged`)

**Total time: ~1â€“2 seconds, completely invisible to the user.**

### 2.4 Data Flow: Weekly Digest

1. `node-cron` fires every Sunday at 7:00 PM (configured timezone)
2. Digest service queries Supabase for all entries from the past 7 days
3. Entries sent to Claude API with digest synthesis prompt
4. Claude returns: top themes, patterns, do-more / do-less recommendations
5. Formatted digest sent via Telegram message
6. Same digest sent via email (Resend API)

---

## 3. Database Schema

### 3.1 Table: `experiences`

The core table. Stores both the raw user text and all structured fields extracted by Claude.

| Field | Type | Description |
|---|---|---|
| `id` | `uuid` | Primary key, auto-generated |
| `created_at` | `timestamptz` | Timestamp of the entry (auto-set by Supabase) |
| `telegram_user_id` | `bigint` | Telegram user ID (enables multi-user support later) |
| `raw_text` | `text` | The exact message the user sent, unmodified |
| `sentiment` | `text` | Extracted sentiment: `positive`, `negative`, or `mixed` |
| `intensity` | `int2` | Intensity score 1â€“10 (1 = mild, 10 = peak experience) |
| `category` | `text` | Primary category (see categories below) |
| `subcategory` | `text` | Optional finer detail (e.g. `coding`, `running`, `meal`) |
| `time_of_day` | `text` | Inferred if mentioned: `morning`, `afternoon`, `evening`, `night` |
| `keywords` | `text[]` | Array of extracted keywords and themes |
| `context_notes` | `text` | Additional context Claude extracted (solo vs social, location, etc.) |
| `extraction_model` | `text` | Which Claude model was used (for future comparison) |

**Valid categories:** `deep_work` Â· `creative` Â· `learning` Â· `social` Â· `physical` Â· `emotional` Â· `other`

### 3.2 Table: `weekly_digests`

Stores a record of every digest sent, for historical reference.

| Field | Type | Description |
|---|---|---|
| `id` | `uuid` | Primary key |
| `created_at` | `timestamptz` | When the digest was generated |
| `week_start` | `date` | Monday of the week this digest covers |
| `week_end` | `date` | Sunday of the week this digest covers |
| `entry_count` | `int4` | Number of experiences logged this week |
| `digest_text` | `text` | The full generated digest content |
| `telegram_sent` | `bool` | Whether Telegram delivery succeeded |
| `email_sent` | `bool` | Whether email delivery succeeded |

### 3.3 Row Level Security (RLS)

> âš ï¸ **Enable RLS on both tables from day one.** Even as a single user, RLS protects your data at the database level and makes adding multi-user support trivial later.

```sql
-- Enable RLS
ALTER TABLE experiences ENABLE ROW LEVEL SECURITY;
ALTER TABLE weekly_digests ENABLE ROW LEVEL SECURITY;
```

---

## 4. Project Structure

```
hedonic/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bot/
â”‚   â”‚   â”œâ”€â”€ index.ts          # Grammy bot setup + webhook registration
â”‚   â”‚   â”œâ”€â”€ handlers.ts       # Message handler â€” main entry point for each message
â”‚   â”‚   â””â”€â”€ responses.ts      # Telegram reply templates
â”‚   â”œâ”€â”€ extraction/
â”‚   â”‚   â”œâ”€â”€ extractor.ts      # Claude API call + extraction logic
â”‚   â”‚   â””â”€â”€ prompts.ts        # All Claude prompts in one place
â”‚   â”œâ”€â”€ digest/
â”‚   â”‚   â”œâ”€â”€ digest.ts         # Weekly digest generation logic
â”‚   â”‚   â”œâ”€â”€ scheduler.ts      # node-cron setup
â”‚   â”‚   â””â”€â”€ email.ts          # Resend email delivery
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ supabase.ts       # Supabase client initialization
â”‚   â”‚   â””â”€â”€ queries.ts        # All database queries in one place
â”‚   â””â”€â”€ index.ts              # App entry point
â”œâ”€â”€ .env.example              # Environment variable template
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ railway.json              # Railway deployment config
```

---

## 5. Claude Extraction Prompt

### 5.1 Design Goal

The extraction prompt is the most critical piece of the system. It must reliably convert a casual, natural-language entry into structured JSON â€” every single time, with no user intervention.

### 5.2 System Prompt (`src/extraction/prompts.ts`)

```
You are an experience extraction engine for a personal wellbeing tracker.
Your job is to extract structured data from a user's natural-language entry.

Always return ONLY valid JSON with this exact shape:
{
  "sentiment": "positive" | "negative" | "mixed",
  "intensity": number (1-10),
  "category": "deep_work" | "creative" | "learning" | "social" | "physical" | "emotional" | "other",
  "subcategory": string | null,
  "time_of_day": "morning" | "afternoon" | "evening" | "night" | null,
  "keywords": string[],
  "context_notes": string | null
}

Rules:
- intensity 8-10 = peak/transformative; 5-7 = notable; 1-4 = mild
- keywords: 2-5 short phrases capturing the essence
- context_notes: solo vs social, where, who with (only if clearly mentioned)
- If the user describes something hard BUT rewarding, score positive with high intensity
- Return ONLY the JSON object. No preamble, no explanation.
```

### 5.3 Example Extraction

**Input:**
```
Just finished a brutal but satisfying 10-mile run this morning. Legs are destroyed
but my head is completely clear â€” best I've felt all week.
```

**Output:**
```json
{
  "sentiment": "positive",
  "intensity": 9,
  "category": "physical",
  "subcategory": "running",
  "time_of_day": "morning",
  "keywords": ["long run", "mental clarity", "physical challenge", "satisfaction"],
  "context_notes": "solo activity, outdoor"
}
```

---

## 6. Weekly Digest

### 6.1 Digest Prompt

The digest prompt receives all entries from the past 7 days as a JSON array and returns a structured, actionable narrative.

```
You are a personal wellbeing analyst reviewing someone's week of logged experiences.
Based on the entries provided, write a weekly digest with this structure:

1. THIS WEEK AT A GLANCE (2-3 sentences, warm and direct tone)
2. YOUR PEAK MOMENTS (top 2-3 highest-intensity positive experiences)
3. PATTERNS I NOTICED (e.g. "Your best moments were all solo, in the morning")
4. DO MORE OF THIS (2-3 specific, concrete recommendations)
5. DO LESS OF THIS (1-2 honest observations, compassionate but direct)
6. QUESTION TO CARRY INTO NEXT WEEK (one thought-provoking question)

Tone: like a thoughtful friend who knows you well â€” honest, warm, specific.
Do not be generic. Reference the actual content of their entries.
```

### 6.2 Digest Delivery

| Channel | Details |
|---|---|
| Telegram | Sent to the same bot chat, formatted with bold headers using Telegram's MarkdownV2 |
| Email | Sent via Resend API â€” plain HTML template, mobile-friendly, no tracking pixels |
| Schedule | Every Sunday at 7:00 PM in the configured timezone (default: `America/Denver`) |
| Minimum entries | Only sends if â‰¥3 entries exist for the week. Otherwise sends an encouraging nudge. |

---

## 7. Environment Variables

All secrets are stored as Railway environment variables. Never commit these to git.

Copy `.env.example` to `.env` for local development.

| Variable | Description |
|---|---|
| `TELEGRAM_BOT_TOKEN` | From @BotFather on Telegram |
| `ANTHROPIC_API_KEY` | From [console.anthropic.com](https://console.anthropic.com) |
| `SUPABASE_URL` | Your Supabase project URL |
| `SUPABASE_SERVICE_KEY` | Service role key (not anon key â€” bypasses RLS for server use) |
| `RESEND_API_KEY` | From [resend.com](https://resend.com) dashboard |
| `DIGEST_EMAIL_TO` | Your email address for digest delivery |
| `DIGEST_EMAIL_FROM` | Verified sender address in Resend |
| `TELEGRAM_USER_ID` | Your Telegram user ID (restricts bot to owner only) |
| `DIGEST_TIMEZONE` | e.g. `America/Denver` |
| `WEBHOOK_SECRET` | Random string for Telegram webhook verification |

---

## 8. Deployment: Railway

### 8.1 Setup Steps

1. Push project to a GitHub repository
2. Create new Railway project â†’ **Deploy from GitHub repo**
3. Add all environment variables in the Railway dashboard
4. Railway auto-detects Node.js and runs `npm run build && npm start`
5. Railway provides a public HTTPS URL â€” use this to register your Telegram webhook

### 8.2 `railway.json`

```json
{
  "$schema": "https://railway.app/railway.schema.json",
  "build": { "builder": "NIXPACKS" },
  "deploy": {
    "startCommand": "node dist/index.js",
    "restartPolicyType": "ON_FAILURE",
    "restartPolicyMaxRetries": 3
  }
}
```

> ðŸ’¡ Railway's Hobby plan ($5/month) is more than sufficient. The bot is webhook-driven (not polling), so it uses almost no resources between messages.

---

## 9. Recommended Build Phases

| Phase | Goal |
|---|---|
| **Phase 1 â€” Core Loop** | Telegram bot receives message â†’ Claude extracts â†’ stores to Supabase â†’ sends `âœ“ Logged` |
| **Phase 2 â€” Digest** | Sunday cron generates and sends digest via Telegram + email |
| **Phase 3 â€” Polish** | Better ack messages, error handling, retry logic, timezone config |
| **Phase 4 â€” Insights** | Query commands (`/week`, `/month`), entry history, category breakdowns |
| **Phase 5 â€” Optional** | pgvector semantic search, multi-user support, web dashboard |

> **Recommendation:** Start with Phase 1 only. Get the logging loop working perfectly before building the digest. The core value of this system is in data quality â€” a clean extraction pipeline is worth more than ten features.

---

## 10. Open Questions & Future Decisions

Intentionally deferred to keep Phase 1 simple. Revisit after 30 days of real use.

- **Timezone handling** â€” hardcode `America/Denver` for now; make configurable in Phase 3
- **Multi-user support** â€” the schema supports it (`telegram_user_id` field exists), but Phase 1 restricts to owner only via `TELEGRAM_USER_ID` env var
- **Entry correction** â€” define a `/delete last` command in Phase 3 for when you log something by mistake
- **Category taxonomy** â€” the 7 categories are a starting point; review after 30 days and adjust based on your actual entries
- **Semantic search** â€” Supabase's `pgvector` extension can power "find entries similar to this feeling" â€” consider in Phase 5

---

*â€” End of Specification â€”*
